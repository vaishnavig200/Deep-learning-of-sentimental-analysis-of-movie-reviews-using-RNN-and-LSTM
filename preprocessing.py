# -*- coding: utf-8 -*-
"""Copy of ML_Project_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bmFYgjqQKKVWltu57cfMh_hrOeQYMDyK
"""

!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz

!tar xvzf aclImdb_v1.tar.gz

import os

def fetch_reviews(path):
  data = []
  #path = 'aclImdb/train/pos/'
  files = [f for f in os.listdir(path)]
  for file in files:
    with open(path+file, "r", encoding='utf8') as f:
      data.append(f.read())

  return data

import pandas as pd

df_train_pos = pd.DataFrame({'review': fetch_reviews('aclImdb/train/pos/'), 'label': 1})
df_train_neg = pd.DataFrame({'review': fetch_reviews('aclImdb/train/neg/'), 'label': 0})

df_test_pos = pd.DataFrame({'review': fetch_reviews('aclImdb/test/pos/'), 'label': 1})
df_test_neg = pd.DataFrame({'review': fetch_reviews('aclImdb/test/neg/'), 'label': 0})

# Merging all df's for data cleaning and preprocessing step.
df = pd.concat([df_train_pos, df_train_neg, df_test_pos, df_test_neg], ignore_index=True)
print("Total reviews in df: ", df.shape)
df.head()

df['review'].isnull().sum()#to remove the null data objects

print("Total Number of positive reviews in data: ", df[df['label']==1].shape[0])
print("Total Number of negative reviews in data: ", df[df['label']==0].shape[0])
print(df['label'])

# sample positive movie review
df[df['label']==1].sample(n=1)['review'].iloc[0]

# sample negative review
df[df['label']==0].sample(n=1)['review'].iloc[0]

# word_count in reviews
word_counts = df['review'].apply(lambda x: len(x.split()))

word_counts.describe()

import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

stop_words = stopwords.words('english')
stop_words.remove('not')
lemmatizer = WordNetLemmatizer()

def data_preprocessing(review):

  # data cleaning
  review = re.sub(re.compile('<.*?>'), '', review) #removing html tags
  review =  re.sub('[^A-Za-z0-9]+', ' ', review) #taking only words

  # lowercase
  review = review.lower()

  # tokenization
  tokens = nltk.word_tokenize(review) # converts review to tokens

  # stop_words removal
  review = [word for word in tokens if word not in stop_words] #removing stop words

  # lemmatization
  review = [lemmatizer.lemmatize(word) for word in review]

  # join words in preprocessed review
  review = ' '.join(review)

  return review

df['preprocessed_review'] = df['review'].apply(lambda review: data_preprocessing(review))
df.head()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['preprocessed_review'])
sequences = tokenizer.texts_to_sequences(df['preprocessed_review'])


max_len = 100  # Adjust as needed
padded_sequences = pad_sequences(sequences, maxlen=max_len)


# Define the LSTM model
vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the reserved 0 index
embedding_dim = 100  # Adjust as needed
lstm_units = 100  # Adjust as needed

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))
model.add(LSTM(units=lstm_units, dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print model summary
print(model.summary())

# Train the model
num_epochs = 5  # Adjust as needed
model.fit(padded_sequences, df['label'], epochs=num_epochs, validation_split=0.5)

# Evaluate the model
loss, accuracyLSTM = model.evaluate(padded_sequences, df['label'])
print(f'Test Loss: {loss}, Test Accuracy: {accuracyLSTM}')

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN
# fixing every word's embedding size to be 32
embedding_dim = 32

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['preprocessed_review'])
sequences = tokenizer.texts_to_sequences(df['preprocessed_review'])


max_len = 100  # Adjust as needed
padded_sequences = pad_sequences(sequences, maxlen=max_len)

# Define the LSTM model
vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the reserved 0 index
embedding_dim = 100  # Adjust as needed
lstm_units = 100  # Adjust as needed
max_len = 100  # Adjust as needed
# Creating a RNN model
RNN_model = Sequential(name="Simple_RNN")
RNN_model.add(Embedding(vocab_size,
						embedding_dim,
						input_length=max_len))

# In case of a stacked(more than one layer of RNN)
# use return_sequences=True
RNN_model.add(SimpleRNN(128,
						activation='tanh',
						return_sequences=False))
RNN_model.add(Dense(1, activation='sigmoid'))

# printing model summary
print(RNN_model.summary())

# Compiling model
RNN_model.compile(
	loss="binary_crossentropy",
	optimizer='adam',
	metrics=['accuracy']
)

# Training the model
#history = RNN_model.fit(x_train_, y_train_,
						#batch_size=64,
						#
num_epochs = 5  # Adjust as needed
RNN_model.fit(padded_sequences, df['label'], epochs=num_epochs, validation_split=0.5)

# Evaluate the model
loss, accuracyRNN = RNN_model.evaluate(padded_sequences, df['label'])
print(f'Test Loss: {loss}, Test Accuracy: {accuracyRNN}')